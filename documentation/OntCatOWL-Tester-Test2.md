# OntCatOWL-Tester: Test 2

The OntCatOWL-Tester Test 2 evaluates the amount of information inferred through successively increasing the quantity of input provided.

Like in [Test 1](https://github.com/unibz-core/OntCatOWL-Tester/blob/main/documentation/OntCatOWL-Tester-Test1.md), only eligible classes are counted and used as possible inputs in Test 2. Eligible classes are the ones with a gUFO ontological category different from the string "other" ([click here](https://github.com/unibz-core/OntCatOWL-Tester/blob/main/documentation/OntCatOWL-Tester-Build.md#ontouml-stereotype-and-gufo-classification) for more information).

All tests implemented in the OntCatOWL-Tester execute OntCatOWL using it as a package. The tests call the OntCatOWL function `run_ontcatowl_tester`. The called function is a lighter version of the regular OntCatOWL function: it implements the same rules and have the same logics, however it does not allow the use of all arguments and does not present information (both at the screen or in files).

## Contents

- [Objectives and Utility](#objectives-and-utility)
- [Execution Procedures](#execution-procedures)
  - [Input and Configuration](#input-and-configuration)
  - [Example](#example)
  - [Execution](#execution)
  - [Output](#output)
- [Execution Instructions](#execution-instructions)

## Objectives and Utility

The aim of this test is to evaluate how information discovery varies with the increasing amount of input provided. Through the analysis of the generated dataset, a user must be able to answer the following questions for general research:

- _On average, how much information a user must provide to a model so that she/he can have more inferred information than asserted information? I.e., at which point does a user has more information discovered than provided?_
- _On average, how much information a user must provide to a model so that she/he can have all information about a model completely known?_
- _On average, how much information a user must provide to a model to discover if it is consistent or not? I.e., at which point a user can detect that a model is inconsistent?_

Regarding utility for specific models, the analysis of the data resulting from Test 2 can answer questions like:

- _Were inconsistencies or incompleteness found in the model?_
- _Do the inferred classifications match the original classifications?_

The Tester does not perform any analysis, it only provides the data to be analyzed further by interested users. We provide the resulting files generated by an execution of Test 2 in a dedicated repository created for hosting this dataset. Any user can access this dataset using [this link](https://github.com/unibz-core/OntCatOWL-Dataset/). Please note that, differently from Test 1, where the execution is always going to generate the same resulting data, in Test 2 different executions are going to generate a different dataset to be evaluated, even when using the same configuration, because of its random selection of the input classes.

## Execution Procedures

### Input and Configuration

Test 2 requires the setting variables for its execution. First, the user must inform if the Tester must consider the models to be tested as complete or incomplete (more information in this link **(create link)**), which is done by editing the variable `is_complete` in the file `__init__.py` file (link to be created). For the Tester, the OntCatOWL's configuration `is_automatic` is always set as true.

The other variables to be configured in the same file are:

- `MINIMUM_ALLOWED_NUMBER_CLASSES`: an integer corresponding to the minimum number of classes that a taxonomy must have to be tested. I.e., if the taxonomy does not have this value, it is skipped by Test 1
- `PERCENTAGE_INITIAL`: a float number corresponding to the first percentage of classes used as input to be tested (e.g., 10, corresponding to 10% of the classes of a taxonomy)
- `PERCENTAGE_FINAL`: a float number corresponding to the last percentage of classes used as input to be tested (e.g., 90, corresponding to 90% of the classes of a taxonomy)
- `PERCENTAGE_RATE`: a float number that represents the rate at which the `PERCENTAGE_INITIAL` is increased up to reach the `PERCENTAGE_FINAL` value (e.g., 10, corresponding to an increase of 10% for each execution round).
- `NUMBER_OF_EXECUTIONS_PER_DATASET_PER_PERCENTAGE`: an integer representing the number of times the same execution (i.e., with a same percentage of input) is performed using the same input quantity, but using different input classes (e.g., 5 executions)

### Example

As an example, consider the following example:

- Two datasets A and B, with A having two taxonomies A1 and A2 with 5 and 20 eligible classes, respectively, and with B having a single taxonomy B1 with eligible 30 classes
- The following values for the configuration variables: `MINIMUM_ALLOWED_NUMBER_CLASSES` = 10, `PERCENTAGE_INITIAL` = 10, `PERCENTAGE_FINAL` = 90, `PERCENTAGE_RATE` = 20, `NUMBER_OF_EXECUTIONS_PER_DATASET_PER_PERCENTAGE` = 5

Here, we have:

- The following percentages of inputs are going to be executed: 10, 30, 50, 70, and 90
- For each of these five percentages, the OntCatOWL is going to be executed 5 times with different classes as input
- A1 will not be tested, as it has only 5 eligible classes, a number that is inferior to the `MINIMUM_ALLOWED_NUMBER_CLASSES`
- A2 is going to be tested with the following quantity of input for each percentage: 2 (10%), 6 (30%), 10 (50%), 14 (70%), and 18 (90%)
- B1 is going to be tested with the following quantity of input for each percentage: 3 (10%), 9 (30%), 15 (50%), 21 (70%), and 27 (90%)
- For 10%, A2 is going to be executed 5 times with different sets of two classes as inputs (e.g., classes A11 and A12 in execution 1, classes A11 and A15 in execution 2, classes A12 and A13 in execution 3, etc.)

In this example, the percentages of classes were not float values, but integers. If the resulting percentage of classes is a decimal number, it is going to be rounder to its nearest integer value.

### Execution

For each possible completeness configuration (complete or incomplete), the Tester creates a different folder inside each dataset folder for storing the test's resulting files. The folders created by Test 2 are _tt002\_ac_ (when `is_complete` equals true) and _tt002\_an_ (when `is_complete` equals false).

Test 2 is executed for each taxonomy (of all datasets) that has more eligible classes than `MINIMUM_ALLOWED_NUMBER_CLASSES`.

For every taxonomy (_ttl_ files generated by the [build function](https://github.com/unibz-core/OntCatOWL-Tester/blob/main/documentation/OntCatOWL-Tester-Build.md)), the Tester creates a list with valid input classes (the eligible classes) in the same way Test 1 does ([click here](https://github.com/unibz-core/OntCatOWL-Tester/blob/main/documentation/OntCatOWL-Tester-Test1.md#execution-procedures) for more information). The input list size is got from the current value of the "percentage" that is going to be tested. If the resulting percentage of classes is a decimal number, it is going to be rounder to its nearest integer value using the Python function `round`.

The current tested percentage value is executed `NUMBER_OF_EXECUTIONS_PER_DATASET_PER_PERCENTAGE` times and, for each of these times, a new input list is going to be created (with the same size but containing different elements). The first percentage being tested is `PERCENTAGE_INITIAL`. After finishing the test of the current percentage, the software increases the current percentage value in `PERCENTAGE_RATE` percentage points and it repeats this process up to when the current percentage value equals `PERCENTAGE_FINAL`.

If OntCatOWL reports at least one inconsistency, the Tester registers it in a specific file (more information in [this link](https://github.com/unibz-core/OntCatOWL-Dataset/blob/main/documentation/OntCatOWL-Dataset-Test2.md#inconsistencies-csv-file)). After that, the tester interrupts the current execution and starts the next one.

### Output

The execution of Test 2 generates of the following files:

- Execution statistics _csv_ files
- Execution times _csv_ files
- Inconsistencies _csv_ File
- Results _yaml_ files
- Results _csv_ files
- Knowledge Matrix _csv_ files
- Divergences _csv_ files

These files are going to be displayed in a structure similar to the one created by Test 1, which is [represented here](https://github.com/unibz-core/OntCatOWL-Tester/blob/main/documentation/OntCatOWL-Tester-Test1.md#resulting-structure). Note, however, that the structure is not going to contain Execution Summary _csv_ Files and that the file's names are going to contain the substring tt002 instead of tt001.

You can find the complete description of all output files generated in Test 2 by accessing its corresponding page at the repository with the OntCatOWL [tests resulting datasets](https://github.com/unibz-core/OntCatOWL-Dataset/blob/main/documentation/OntCatOWL-Dataset-Test2.md).

## Execution Instructions

It is necessary to first execute the Tester's build function to create the structure for the tests. For running the OntCatOWL-Tester's build function, follow the instructions provided in [this link](https://github.com/unibz-core/OntCatOWL-Tester/blob/main/documentation/OntCatOWL-Tester-Build.md#execution-instructions).

For executing the OntCatOWL-Tester Test 2, first you need to set the configuration's values in the [file](https://github.com/unibz-core/OntCatOWL/blob/main/ontcatowl/__init__.py) `__init__.py`. After that, use the following command:

```txt
python ./src/ontcatowl_tester.py -r2
```

Note: the instructions here provided may not work properly in the Tester's current implementationâ€”please refer to [issue #14](https://github.com/unibz-core/OntCatOWL-Tester/issues/14).
